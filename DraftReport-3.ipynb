{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# It's Raining Cats and Dogs\n",
    "   \n",
    "##  Lab Assignment Three: Exploring Image Data \n",
    "   \n",
    "### Justin Ledford, Luke Wood, Traian Pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Business Understanding\n",
    "\n",
    "(10 points total).  \n",
    "[10 points] Give an overview of the dataset. Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). Why is this data important and how will you know if you have gathered useful knowledge from the dataset? Be specific and use your own words to describe the aspects of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data set we are analyzing has over 25000 evenly distributed pictures of dogs and cats. We found it on https://www.kaggle.com/c/dogs-vs-cats and decided it would be a good set to practice processing and analyzing image data due to the clear classification metric of dogs|cats and the large number of high quality photos.\n",
    "   \n",
    "This data was originally collected for the purpose of a competition hosted by Kaggle to see who could create the most accurate algorithm that could distinguish between pictures of dogs and cats. Petfined.com are the original owners of the data, and they donated over 3 million pictures of sheltered animals that have been hand classified by them. Because of this, we are highly confident in the data, as the vast majority of humans can successfully identify between a dog and a cat. \n",
    "   \n",
    "The competition itself is long over, but the page still remains for people to play and experiment with the data. This data is important due to potential it holds to help develop and train algorithms that could be used to evolve image recognition software. We can progressively check how useful the data we get from the set is by comparing it against our own classification and seeing if our own conclusions (i.e. if an image classifies a cat or not) match with the information we collect from it. \n",
    "   \n",
    "We are not expecting perfect results as cats and dogs can look fairly similar at times, and the black and white color format of the pictures already limits easily differential characteristics between the two animals, such as fur color. However, we do believe that due to the large number and quality of the pictures, we can definitely expect to get a large amount of succesful results when processing and analyzing the data.\n",
    "\n",
    "### Applications\n",
    "This dataset could be used to train an image classifier that could be used by hotels or apartment that only allow one type of pet.  For example, Luke's apartment complex only allows cats to stay in the homes so the complex could theoretically place cameras in the hallway and test to see which type of pets are being brought in.  This would allow them to more efficiently enforce the no dogs rule which is frequently broken.  \n",
    "\n",
    "The data set could also be used for cataloging by a pet shelter to allow owners to search for cats or dogs similar to one they are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Preparation \n",
    "(20 points total)\n",
    "\n",
    "[5 points] Read in your images data as numpy arrays. Resize and recolor images as necessary. (DONE)\n",
    "   \n",
    "[10 points] Linearize the images to create a table of 1-D image features (each row should be one image). (DONE)\n",
    "   \n",
    "[5 points] Visualize several images. (DONE)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import plotly\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "w,h = 150,150\n",
    "def wildcard_to_df(fpath, greyscale=True):\n",
    "    X = []\n",
    "    labels = []\n",
    "\n",
    "    # Image sizes\n",
    "    h, w = 150, 150 \n",
    "\n",
    "    for img_file in glob.iglob(fpath):\n",
    "        # Read in img as greyscale\n",
    "        with Image.open(img_file).convert(\"L\") as img:\n",
    "\n",
    "            # Keep labels for each image in separate array\n",
    "            if 'cat' in img_file:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "            # Concatenate RGB into one row and collect\n",
    "            X.append(np.concatenate(np.array(img)))\n",
    "\n",
    "    X = np.array(X)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    #convert to DF\n",
    "    return pd.DataFrame(data=X, index=labels), X, labels\n",
    "\n",
    "df, X, label = wildcard_to_df('data/barred/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3ac47566367b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplot_gallery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# defaults to showing a 3 by 6 subset of the faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Visualize some images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3ac47566367b>\u001b[0m in \u001b[0;36mplot_gallery\u001b[0;34m(images, h, w, n_row, n_col)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_row\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#plt.title(titles[i], size=12)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMkAAADWCAYAAAB7RqZLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11464e2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# a helper plotting function (modified Larson's code)\n",
    "def plot_gallery(images, h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        #plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "plot_gallery(X, h, w) # defaults to showing a 3 by 6 subset of the faces\n",
    "\n",
    "# Visualize some images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Reduction \n",
    "(60 points total)\n",
    "\n",
    "[10 points] Perform linear dimensionality reduction of the images using principal components analysis. Visualize the explained variance of each component. How many dimensions are required to adequately represent your image data?\n",
    "   \n",
    "[10 points] Perform non-linear dimensionality reduction of your image data. Compare the representation using non-linear dimensions to using linear dimensions. Do you prefer one method over another? Why?\n",
    "   \n",
    "[20 points] Perform feature extraction upon the images using any feature extraction technique (e.g., gabor filters, ordered gradients, etc.).\n",
    "   \n",
    "[20 points] Visualize the differences between instances in each target class. For example, a heat map of the pairwise differences among all instances is likely appropriate for you data. Do you think the features extracted are appropriate for classification on your dataset? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 300\n",
    "pca = PCA(n_components=n_components)\n",
    "%time pca.fit(X)\n",
    "eigens = pca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 300\n",
    "pca = PCA(n_components=n_components)\n",
    "%time pca.fit(X)\n",
    "eigens = pca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(eigens, h, w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The issues with the current PCA\n",
    "After running principal component analysis on our preprocessed images, we realized that we had a huge issue!  Almost every single component represented either dark edges or light edges.  These edges are not at all representative of the images themselves, but instead show which images we preprocessed to have dark borders or light borders.  We realized that we needed to find a different way to format the images so we instead tried cropping the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropped Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#convert to DF\n",
    "df, X, labels = wildcard_to_df('data/cropped/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# a helper plotting function (modified Larson's code)\n",
    "def plot_gallery(images, h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        #plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "plot_gallery(X, h, w) # defaults to showing a 3 by 6 subset of the faces\n",
    "\n",
    "# Visualize some images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 300\n",
    "pca = PCA(n_components=n_components)\n",
    "%time pca.fit(X)\n",
    "eigens = pca.components_.reshape((n_components, h, w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_gallery(eigens, h, w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code by Larson, slightly changed to our data. Not final but cool to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_explained_variance(pca):\n",
    "    from plotly.graph_objs import Scatter, Marker, Layout, XAxis, YAxis, Bar, Line\n",
    "    plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "    \n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(explained_var)\n",
    "    \n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Bar(y=explained_var, name='individual explained variance'),\n",
    "                 Scatter(y=cum_var_exp, name='cumulative explained variance')\n",
    "            ],\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal components'), yaxis=YAxis(title='Explained variance ratio'))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_explained_variance(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(eigens, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_image(trans_obj,org_features):\n",
    "    low_rep = trans_obj.transform(org_features)\n",
    "    rec_image = trans_obj.inverse_transform(low_rep)\n",
    "    return low_rep, rec_image\n",
    "    \n",
    "idx_to_reconstruct = 4    \n",
    "low_dimensional_representation, reconstructed_image = reconstruct_image(pca,X[idx_to_reconstruct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X[idx_to_reconstruct].reshape((h, w)), cmap=plt.cm.gray)\n",
    "plt.title('Original')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(reconstructed_image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "plt.title('Reconstructed from Full PCA')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#randomized pca\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "n_components = 300\n",
    "print (\"Extracting the top %d eigenfaces from %d faces\" % (\n",
    "    n_components, X.shape[0]))\n",
    "\n",
    "rpca = RandomizedPCA(n_components=n_components)\n",
    "%time rpca.fit(X)\n",
    "eigenfaces = rpca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(eigenfaces, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#kernel\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "n_components = 300\n",
    "print (\"Extracting the top %d eigenfaces from %d faces\" % (n_components, X.shape[0]))\n",
    "\n",
    "kpca = KernelPCA(n_components=n_components, kernel='rbf', \n",
    "                fit_inverse_transform=True, gamma=15) # very sensitive to the gamma parameter\n",
    "%time kpca.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "from ipywidgets import widgets  # make this interactive!\n",
    "# compare the different methods\n",
    "\n",
    "def plt_reconstruct(idx_to_reconstruct):\n",
    "    idx_to_reconstruct = np.round(idx_to_reconstruct)\n",
    "    \n",
    "    reconstructed_image = pca.inverse_transform(pca.transform(X[idx_to_reconstruct]))\n",
    "    reconstructed_image_rpca = rpca.inverse_transform(rpca.transform(X[idx_to_reconstruct]))\n",
    "    reconstructed_image_kpca = kpca.inverse_transform(kpca.transform(X[idx_to_reconstruct]))\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15,7))\n",
    "    \n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(X[idx_to_reconstruct].reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Original')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(reconstructed_image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Full PCA')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(reconstructed_image_rpca.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Randomized PCA')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(reconstructed_image_kpca.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Kernel PCA')\n",
    "    plt.grid()\n",
    "\n",
    "plt_reconstruct(4)\n",
    "#widgets.interact(plt_reconstruct,idx_to_reconstruct=(0,n_samples-1,1),__manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#image processing\n",
    "from skimage.io import imshow\n",
    "\n",
    "idx_to_reconstruct = int(np.random.rand(1)*len(X))\n",
    "img  = X[idx_to_reconstruct].reshape((h,w))\n",
    "imshow(img)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from skimage.filters import sobel_h, sobel_v\n",
    "\n",
    "gradient_mag = np.sqrt(sobel_v(img)**2 + sobel_h(img)**2 ) \n",
    "imshow(gradient_mag)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#daisy\n",
    "from skimage.feature import daisy\n",
    "\n",
    "# lets first visualize what the daisy descripto looks like\n",
    "features, img_desc = daisy(img,step=40, radius=10, rings=3, histograms=5, orientations=8, visualize=True)\n",
    "imshow(img_desc)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# now let's understand how to use it\n",
    "features = daisy(img,step=10, radius=10, rings=2, histograms=4, orientations=8, visualize=False)\n",
    "print(features.shape)\n",
    "print(features.shape[0]*features.shape[1]*features.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a function to tak in the row of the matric and return a new feature\n",
    "def apply_daisy(row,shape):\n",
    "    feat = daisy(row.reshape(shape),step=10, radius=10, rings=2, histograms=6, orientations=8, visualize=False)\n",
    "    return feat.reshape((-1))\n",
    "\n",
    "%time test_feature = apply_daisy(X[3],(h,w))\n",
    "test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "0.019 * len(X) # approximate how long it may run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# apply to entire data, row by row,\n",
    "# takes about a minute to run\n",
    "%time daisy_features = np.apply_along_axis(apply_daisy, 1, X, (h,w))\n",
    "print(daisy_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "# find the pairwise distance between all the different image features\n",
    "%time dist_matrix = pairwise_distances(daisy_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# find closest image to current image\n",
    "idx1 = 112\n",
    "distances = copy.deepcopy(dist_matrix[idx1,:])\n",
    "distances[idx1] = np.infty # dont pick the same image!\n",
    "idx2 = np.argmin(distances)\n",
    "\n",
    "plt.figure(figsize=(7,10))\n",
    "plt.subplot(1,2,1)\n",
    "imshow(X[idx1].reshape((h,w)))\n",
    "plt.title(\"Original Image\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "imshow(X[idx2].reshape((h,w)))\n",
    "plt.title(\"Closest Image\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import fixed\n",
    "from ipywidgets import widgets  # make this interactive!\n",
    "\n",
    "# put it together inside a nice widget\n",
    "def closest_image(dmat,idx1):\n",
    "    distances = copy.deepcopy(dmat[idx1,:]) # get all image distances\n",
    "    distances[idx1] = np.infty # dont pick the same image!\n",
    "    idx2 = np.argmin(distances)\n",
    "    \n",
    "    distances[idx2] = np.infty\n",
    "    idx3 = np.argmin(distances)\n",
    "    \n",
    "    plt.figure(figsize=(10,16))\n",
    "    plt.subplot(1,3,1)\n",
    "    imshow(X[idx1].reshape((h,w)))\n",
    "    plt.title(\"Original Image \")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    imshow(X[idx2].reshape((h,w)))\n",
    "    plt.title(\"Closest Image  \")\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    imshow(X[idx3].reshape((h,w)))\n",
    "    plt.title(\"Next Closest Image \")\n",
    "    plt.grid()\n",
    "\n",
    "#closest_image(idx1)\n",
    "widgets.interact(closest_image,idx1=(0,1500-1,1),dmat=fixed(dist_matrix),__manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#garbor\n",
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "from scipy import stats\n",
    "\n",
    "# prepare filter bank kernels\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta,\n",
    "                                          sigma_x=sigma, sigma_y=sigma))\n",
    "            kernels.append(kernel)\n",
    "\n",
    "            \n",
    "# compute the filter bank and take statistics of image\n",
    "def compute_gabor(row, kernels, shape):\n",
    "    feats = np.zeros((len(kernels), 4), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(row.reshape(shape), kernel, mode='wrap')\n",
    "        _,_,feats[k,0],feats[k,1],feats[k,2],feats[k,3] = stats.describe(filtered.reshape(-1))\n",
    "        # mean, var, skew, kurt\n",
    "        \n",
    "    return feats.reshape(-1)\n",
    "\n",
    "idx_to_reconstruct = int(np.random.rand(1)*len(X))\n",
    "\n",
    "gabr_feature = compute_gabor(X[idx_to_reconstruct], kernels, (h,w))\n",
    "gabr_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# takes ~3 minutes to run entire dataset\n",
    "%time gabor_stats = np.apply_along_axis(compute_gabor, 1, X, kernels, (h,w))\n",
    "print(gabor_stats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "# find the pairwise distance between all the different image features\n",
    "%time dist_matrix = pairwise_distances(gabor_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "widgets.interact(closest_image,idx1=(0,1500-1,1),dmat=fixed(dist_matrix),__manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reverse Image Searching\n",
    "We decided to simulate the scenario of someone searching for a pet in a shelter.  We used the images in our dataset to represent animals currently in the shelter, and I chose a picture of a cat from online to represent the cat I am searching for. \n",
    "\n",
    "Here is the type of cat I am looking for:\n",
    "\n",
    "![alt text](img/orange-cat.jpg)\n",
    "\n",
    "I then ran this to reformat the image to our format and added it to our dataset.  This gave us this image:\n",
    "\n",
    "![alt text](img/orange-cat-cropped.jpg)\n",
    "\n",
    "Here is the code used to find similar cats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets narrow this down to ju\n",
    "def search_for_cat(img_path):\n",
    "    df0,X0 =wildcard_to_df(img_path)\n",
    "    complete_gabor_stats = np.concatenate((gabor_stats,compute_gabor(X0,kernels,(h,w))))\n",
    "    distances = pairwise_distances(complete_gabor_stats)\n",
    "    closest_image(distances,1000)\n",
    "    \n",
    "search_for_cat('img/orange-cat-cropped.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the differences between some instances in each class we decided to plot the pairwise differences computed from the gabor filters in heatmaps. We choose a sample of 35 images from each class to compare to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_size = 30\n",
    "cat_sample_indices = [i for i,x in enumerate(labels) if x == 0][:sample_size]\n",
    "dog_sample_indices = [i for i,x in enumerate(labels) if x == 1][:sample_size]\n",
    "\n",
    "cat_dist = dist_matrix[:, cat_sample_indices][cat_sample_indices, :]\n",
    "dog_dist = dist_matrix[:, dog_sample_indices][dog_sample_indices, :]\n",
    "\n",
    "max_dist = np.amax(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cat_dist, vmax=max_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that there are many images with relatively small differences, especially from images 9-25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(dog_dist, vmax=max_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the same trend in this heatmap as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.heatmap(dist_matrix[:,np.arange(0,sample_size)][np.arange(0,sample_size),:],\n",
    "                vmax=max_dist)\n",
    "graph_labels = [\"dog\" if i == 1 else \"cat\" for i in labels[:sample_size]]\n",
    "g.set(xticklabels=graph_labels, yticklabels=reversed(graph_labels))\n",
    "#plt.yticks(rotation=30)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting instances from both classes shows higher differences than plotting instances from the same class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
